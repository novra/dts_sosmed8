{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport IPython","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:00:13.732102Z","iopub.execute_input":"2022-07-26T15:00:13.732825Z","iopub.status.idle":"2022-07-26T15:00:13.739791Z","shell.execute_reply.started":"2022-07-26T15:00:13.732783Z","shell.execute_reply":"2022-07-26T15:00:13.738696Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## Read data dari github\ndata = pd.read_csv('https://raw.githubusercontent.com/novra/dts_sosmed8/main/tweets.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:00:47.654620Z","iopub.execute_input":"2022-07-26T15:00:47.655123Z","iopub.status.idle":"2022-07-26T15:00:48.040331Z","shell.execute_reply.started":"2022-07-26T15:00:47.655078Z","shell.execute_reply":"2022-07-26T15:00:48.039216Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Menghitung jumlah masing-masing target\ntarget = list(set(data['target']))\njumlah_target = []\nfor i in target:\n  jumlah_target.append(list(data['target']).count(i))\n\n#visualisasi jumlah keyword\nwarna = np.array(['hotpink', 'cornflowerblue'])\nplt.bar(target, jumlah_target, color=warna)\nplt.title(\"Distribusi kelas target\")\n## Menampilkan label pada grafik\nfor i in range(len(target)):\n    plt.text(i, jumlah_target[i], jumlah_target[i], ha = 'center')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:00:51.598158Z","iopub.execute_input":"2022-07-26T15:00:51.598672Z","iopub.status.idle":"2022-07-26T15:00:51.916451Z","shell.execute_reply.started":"2022-07-26T15:00:51.598626Z","shell.execute_reply":"2022-07-26T15:00:51.915418Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## mendefinisikan fungsi untuk clean data\ndef clean_data(teks):\n    # Mengubah semua huruf menjadi huruf kecil\n    teks = teks.lower()\n    # Menghapus www.* atau https?://*\n    teks = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',teks)\n    # Menghapus tanda #\n    teks = re.sub(r'#([^\\s]+)', r'\\1', teks)\n    # Menghapus tanda baca\n    teks = re.sub(r'[^\\w\\s]',' ', teks)\n    # Menghapus angka\n    teks = re.sub(r'[\\d-]', '', teks)\n    # Menghapus spasi berlebih\n    teks = re.sub('[\\s]+', ' ', teks)\n    # Menghapus tanda \\, ', dan \"\n    teks = teks.strip('\\'\"')\n    \n    # Pembersihan kata\n    words = teks.split()\n    tokens=[]\n    for ww in words:\n        # Memisahkan kata berulang\n        for w in re.split(r'[-/\\s]\\s*', ww):\n            # Menghapus huruf berulang yang lebih dari dua kali\n            pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n            w = pattern.sub(r\"\\1\\1\", w)\n            w = w.strip('\\'\"?,.')\n            # Memeriksa apakah suatu kata terbentuk dari minimal dua huruf\n            val = re.search(r\"^[a-zA-Z][a-zA-Z][a-zA-Z]*$\", w)\n            if w == \"rt\" or val is None:\n                continue\n            else:\n                tokens.append(w.lower())\n    \n    teks = \" \".join(tokens)  \n    return teks\n\n# clean data teks\ndata['text'] = data['text'].map(lambda x: clean_data(x))\nteks = data[data['text'].apply(lambda x: len(x.split()) >=1)]\nteks = np.array(data['text'])\n\n#One hot encoding pada data target\ntarget = np.array(pd.get_dummies(data['target']))","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:00:54.085630Z","iopub.execute_input":"2022-07-26T15:00:54.086242Z","iopub.status.idle":"2022-07-26T15:00:56.036946Z","shell.execute_reply.started":"2022-07-26T15:00:54.086207Z","shell.execute_reply":"2022-07-26T15:00:56.035958Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Pemisahan data training & data testing\ndata_train,data_test,label_train,label_test = train_test_split(teks, target, test_size=0.2,\n                                                               stratify=target, random_state=7)","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:01:04.298655Z","iopub.execute_input":"2022-07-26T15:01:04.299408Z","iopub.status.idle":"2022-07-26T15:01:04.385169Z","shell.execute_reply.started":"2022-07-26T15:01:04.299362Z","shell.execute_reply":"2022-07-26T15:01:04.384032Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nbert_tokenizer = BertTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')\nbert_model = TFBertModel.from_pretrained(\"cross-encoder/ms-marco-TinyBERT-L-2-v2\", trainable=False, from_pt=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:01:06.562766Z","iopub.execute_input":"2022-07-26T15:01:06.564086Z","iopub.status.idle":"2022-07-26T15:01:20.860227Z","shell.execute_reply.started":"2022-07-26T15:01:06.564040Z","shell.execute_reply":"2022-07-26T15:01:20.859069Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Pendefinisian fungsi untuk melakukan tokenisasi pada satu data\ndef tokenisasi(teks):\n      encode_dict = bert_tokenizer(teks,\n                                   add_special_tokens = True,\n                                   max_length = 80,\n                                   padding = 'max_length',\n                                   truncation = True,\n                                   return_attention_mask = True,\n                                   return_tensors = 'tf',)\n\n      tokenID = encode_dict['input_ids']\n      attention_mask = encode_dict['attention_mask']\n\n      return tokenID, attention_mask\n\n# Pendefinisian fungsi untuk mengambil hasil tokenisasi pada semua data\ndef create_input(data):\n    tokenID, input_mask = [], []\n    for teks in data:\n        token, mask = tokenisasi(teks)\n        tokenID.append(token)\n        input_mask.append(mask)\n    \n    return {'input_ids': np.asarray(tokenID, dtype=np.int32).reshape(-1, 80), \n            'attention_mask': np.asarray(input_mask, dtype=np.int32).reshape(-1, 80)}","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:01:20.862505Z","iopub.execute_input":"2022-07-26T15:01:20.863595Z","iopub.status.idle":"2022-07-26T15:01:20.878605Z","shell.execute_reply.started":"2022-07-26T15:01:20.863554Z","shell.execute_reply":"2022-07-26T15:01:20.877034Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Membuat tokenID untuk X_train dan X_test\nX_train = create_input(data_train)\nX_test = create_input(data_test)\n\n#Mengambil representasi teks dari encoder layer ke 12 dari model BERT\nX_train = bert_model(**X_train)[0]\nX_test = bert_model(**X_test)[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:01:20.880491Z","iopub.execute_input":"2022-07-26T15:01:20.881495Z","iopub.status.idle":"2022-07-26T15:01:32.738956Z","shell.execute_reply.started":"2022-07-26T15:01:20.881435Z","shell.execute_reply":"2022-07-26T15:01:32.737805Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from keras_tuner.tuners import BayesianOptimization\n#Mendefinisikan fungsi untuk klasifikasi dengan model hybrid CNN-GRU menggunakan beberapa kandidat hyperparameter\ndef cnn_gru(hp):\n    #Input layer\n    input = keras.layers.Input(shape=(80, 128))\n\n    #Convolution layer\n    cnn = keras.layers.Conv1D(filters = hp.Int('filters',\n                                                min_value = 200, \n                                                max_value = 300, \n                                                step = 50),\n                                  kernel_size = hp.Int('kernel_size',\n                                                min_value = 3, \n                                                max_value = 5, \n                                                step = 1),\n                                  activation='relu',\n                                  kernel_regularizer = keras.regularizers.l2(hp.Choice('kernel_cnn',\n                                                                                       values = [0.01, 0.001])))(input)\n\n    #Max Pooling layer\n    maxpool = keras.layers.MaxPooling1D(pool_size=2)(cnn)\n\n    #GRU layer\n    gru = keras.layers.GRU(units = hp.Int('units',\n                                             min_value = 100,\n                                             max_value = 200,\n                                             step = 50),\n                                      kernel_regularizer=keras.regularizers.l2(hp.Choice('kernel_regularizer',\n                                                                                         values = [0.01, 0.001])),\n                                      recurrent_regularizer=keras.regularizers.l2(hp.Choice('rec_regularizer',\n                                                                                            values = [0.01, 0.001])))(maxpool)\n    #Output layer\n    output = keras.layers.Dense(2, activation='softmax',\n                                kernel_regularizer=keras.regularizers.l2(hp.Choice('kernel_dense', values = [0.01, 0.001])))(gru)\n    model = keras.models.Model(inputs=input, outputs=output)\n\n    model.compile(optimizer = keras.optimizers.Adam(1e-3),\n                  loss ='categorical_crossentropy',\n                  metrics=['accuracy'])\n   \n    return model\n\n# Pendefinisian Callback\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n#Hyperparameter tuning menggunakan bayesian optimization dengan\n#banyak percobaan kombinasi hyperparameter sebanyak 10\ntuner = BayesianOptimization(cnn_gru,\n                             objective = 'val_accuracy', \n                             max_trials = 10,\n                             directory = '/content/Hasil',\n                             project_name = 'Sentimen-CNN-GRU',\n                             overwrite = True)\n\ntuner.search(X_train, label_train,\n             batch_size=64, epochs=50,\n             validation_data=(X_test, label_test),\n             callbacks=[early_stop, ClearTrainingOutput()])\n\n# Mendapatkan model terbaik dari 10 percobaan bayesian\nmodel = tuner.get_best_models()[0]","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:14:33.981053Z","iopub.execute_input":"2022-07-26T15:14:33.981512Z","iopub.status.idle":"2022-07-26T15:20:48.906550Z","shell.execute_reply.started":"2022-07-26T15:14:33.981477Z","shell.execute_reply":"2022-07-26T15:20:48.905400Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Retrain model pada data test\nhistory = model.fit(X_train, label_train,\n                    batch_size=32, epochs=50,\n                    validation_data=(X_test, label_test),\n                    callbacks=[early_stop])","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:20:48.908769Z","iopub.execute_input":"2022-07-26T15:20:48.909107Z","iopub.status.idle":"2022-07-26T15:21:18.456481Z","shell.execute_reply.started":"2022-07-26T15:20:48.909077Z","shell.execute_reply":"2022-07-26T15:21:18.455068Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Mendapatkan kinerja model\ny_pred = np.argmax(model.predict(X_test), axis=1)\ny = np.argmax(label_test, axis=1)\nprint('accuracy: ', accuracy_score(y, y_pred),\n      '\\nprecicion: ', precision_score(y, y_pred),\n      '\\nrecall: ', recall_score(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:21:18.458270Z","iopub.execute_input":"2022-07-26T15:21:18.458721Z","iopub.status.idle":"2022-07-26T15:21:18.947023Z","shell.execute_reply.started":"2022-07-26T15:21:18.458679Z","shell.execute_reply":"2022-07-26T15:21:18.945843Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# plot grafik akurasi\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# plot grafik loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-26T15:21:18.949919Z","iopub.execute_input":"2022-07-26T15:21:18.950625Z","iopub.status.idle":"2022-07-26T15:21:19.347449Z","shell.execute_reply.started":"2022-07-26T15:21:18.950579Z","shell.execute_reply":"2022-07-26T15:21:19.346507Z"},"trusted":true},"execution_count":20,"outputs":[]}]}