# -*- coding: utf-8 -*-
"""Copy of Project DTS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u2wKVRSRI_W-f-3If8DihPDL_qZuCcxb
"""

pip install tweet-preprocessor

import re
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.model_selection import train_test_split
import IPython
import nltk
from nltk.tokenize import TweetTokenizer
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import preprocessor as p
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

## Read data dari github
data = pd.read_csv('https://raw.githubusercontent.com/novra/dts_sosmed8/main/tweets.csv')
data.head()

# Menghitung jumlah masing-masing target
target = list(set(data['target']))
jumlah_target = []
for i in target:
  jumlah_target.append(list(data['target']).count(i))

#visualisasi jumlah keyword
warna = np.array(['hotpink', 'cornflowerblue'])
plt.bar(target, jumlah_target, color=warna)
plt.title("Distribusi kelas target")
## Menampilkan label pada grafik
for i in range(len(target)):
    plt.text(i, jumlah_target[i], jumlah_target[i], ha = 'center')

plt.show()

#preprocessing
def preprocess_tweet(row):
    text = row['text']
    text = p.clean(text)
    return text

data['text'] = data.apply(preprocess_tweet, axis=1)

def preprocess_data(data):
  #menghapus angka
  data = data.astype(str).str.replace('\d+', '', regex=True)
  #menjadikan lowercase
  lower_text = data.str.lower()
  #lemmatize
  lemmatizer = nltk.stem.WordNetLemmatizer()
  #tokenize
  w_tokenizer =  TweetTokenizer()

  def lemmatize_text(text):
      return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]
  def remove_punctuation(words):
      new_words = []
      for word in words:
          new_word = re.sub(r'[^\w\s]', '', (word))
          if new_word != '':
            new_words.append(new_word)
      return new_words

  words = lower_text.apply(lemmatize_text)
  words = words.apply(remove_punctuation)
  return pd.DataFrame(words)

data['text'] = preprocess_data(data['text'])

stop_words = set(stopwords.words('english'))
data['text'] = data['text'].apply(lambda x: [item for item in x if item not in stop_words])

#join tokenize data
data['text'] = data['text'].apply(lambda x: ' '.join(x))
data

teks = np.array(data['text'])

#One hot encoding pada data target
target = np.array(pd.get_dummies(data['target']))

# Pemisahan data training & data testing
data_train,data_test,label_train,label_test = train_test_split(teks, target, test_size=0.2,
                                                               stratify=target, random_state=7)

pip install transformers

from transformers import BertTokenizer, TFBertModel
bert_tokenizer = BertTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L-2-v2')
bert_model = TFBertModel.from_pretrained("cross-encoder/ms-marco-TinyBERT-L-2-v2", trainable=False, from_pt=True)

# Pendefinisian fungsi untuk melakukan tokenisasi pada satu data
def tokenisasi(teks):
      encode_dict = bert_tokenizer(teks,
                                   add_special_tokens = True,
                                   max_length = 80,
                                   padding = 'max_length',
                                   truncation = True,
                                   return_attention_mask = True,
                                   return_tensors = 'tf',)

      tokenID = encode_dict['input_ids']
      attention_mask = encode_dict['attention_mask']

      return tokenID, attention_mask

# Pendefinisian fungsi untuk mengambil hasil tokenisasi pada semua data
def create_input(data):
    tokenID, input_mask = [], []
    for teks in data:
        token, mask = tokenisasi(teks)
        tokenID.append(token)
        input_mask.append(mask)
    
    return {'input_ids': np.asarray(tokenID, dtype=np.int32).reshape(-1, 80), 
            'attention_mask': np.asarray(input_mask, dtype=np.int32).reshape(-1, 80)}

# Membuat tokenID untuk X_train dan X_test
X_train = create_input(data_train)
X_test = create_input(data_test)

#Mengambil representasi teks dari encoder layer ke 12 BERT
X_train = bert_model(**X_train)[0]
X_test = bert_model(**X_test)[0]

pip install -q -U keras-tuner

from keras_tuner.tuners import BayesianOptimization

#Mendefinisikan fungsi untuk klasifikasi dengan model hybrid CNN-LSTM menggunakan beberapa kandidat hyperparameter
def cnn_gru(hp):
    #Input layer
    input = keras.layers.Input(shape=(80, 128))

    #Convolution layer
    cnn = keras.layers.Conv1D(filters = hp.Int('filters',
                                                min_value = 200, 
                                                max_value = 300, 
                                                step = 50),
                                  kernel_size = hp.Int('kernel_size',
                                                min_value = 3, 
                                                max_value = 5, 
                                                step = 1),
                                  activation='relu',
                                  kernel_regularizer = keras.regularizers.l2(hp.Choice('kernel_cnn',
                                                                                       values = [0.01, 0.001])))(input)

    #Max Pooling layer
    maxpool = keras.layers.MaxPooling1D(pool_size=2)(cnn)

    #LSTM layer
    lstm = keras.layers.LSTM(units = hp.Int('units',
                                             min_value = 100,
                                             max_value = 200,
                                             step = 50),
                             kernel_regularizer=keras.regularizers.l2(hp.Choice('kernel_regularizer',
                                                                                values = [0.01, 0.001])),
                             recurrent_regularizer=keras.regularizers.l2(hp.Choice('rec_regularizer',
                                                                                   values = [0.01, 0.001])))(maxpool)
    #Output layer
    output = keras.layers.Dense(2, activation='softmax',
                                kernel_regularizer=keras.regularizers.l2(hp.Choice('kernel_dense', values = [0.01, 0.001])))(lstm)
    model = keras.models.Model(inputs=input, outputs=output)

    model.compile(optimizer = keras.optimizers.Adam(1e-3),
                  loss ='categorical_crossentropy',
                  metrics=['accuracy'])
   
    return model

# Pendefinisian Callback
class ClearTrainingOutput(tf.keras.callbacks.Callback):
    def on_train_end(*args, **kwargs):
        IPython.display.clear_output(wait = True)

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

#Hyperparameter tuning menggunakan bayesian optimization
tuner = BayesianOptimization(cnn_gru,
                             objective = 'val_accuracy', 
                             max_trials = 10,
                             directory = '/content/Hasil',
                             project_name = 'CNN-GRU',
                             overwrite = True)

tuner.search(X_train, label_train,
             batch_size=32, epochs=50,
             validation_split=0.2,
             callbacks=[early_stop, ClearTrainingOutput()])

# Mendapatkan model terbaik
model = tuner.get_best_models()[0]

model.summary()

# Retrain model pada data test
history = model.fit(X_train, label_train,
                    batch_size=64, epochs=50,
                    validation_data=(X_test, label_test),
                    callbacks=[early_stop])

# Mendapatkan kinerja model
y_pred = np.argmax(model.predict(X_test), axis=1)
y = np.argmax(label_test, axis=1)
print('accuracy: ', accuracy_score(y, y_pred),
      '\nprecicion: ', precision_score(y, y_pred),
      '\nrecall: ', recall_score(y, y_pred))

# plot grafik akurasi
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
# plot grafik loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()