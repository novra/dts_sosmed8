{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_data tweet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-p8bretoQTj",
        "outputId": "e38f1071-c1fe-44f3-8ffb-30ef4be42ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install tweet-preprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load Data\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "aZ_Z-WF6oXHc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('tweets.csv',encoding='utf8')\n",
        "tweets\n",
        "\n"
      ],
      "metadata": {
        "id": "Rn91aTPd_ujB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot('target', data=tweets)\n",
        "plt.title('Perbandingan Data Setiap Kelas')"
      ],
      "metadata": {
        "id": "RVaceGGpow_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['target'].value_counts()"
      ],
      "metadata": {
        "id": "q1Gg-EdHAILX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "def preprocess_tweet(row):\n",
        "    text = row['text']\n",
        "    text = p.clean(text)\n",
        "    return text\n",
        "\n",
        "tweets['textprep'] = tweets.apply(preprocess_tweet, axis=1)"
      ],
      "metadata": {
        "id": "6UsRK52KALIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "oX-vc20aAOUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        " #menghapus angka\n",
        " data = data.astype(str).str.replace('\\d+', '', regex=True)\n",
        " #menjadikan lowercase\n",
        " lower_text = data.str.lower()\n",
        " #lemmatize\n",
        " lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        " #tokenize\n",
        " w_tokenizer =  TweetTokenizer()\n",
        "\n",
        " def lemmatize_text(text):\n",
        "  return [(lemmatizer.lemmatize(w)) for w \\\n",
        "                       in w_tokenizer.tokenize((text))]\n",
        " def remove_punctuation(words):\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "      new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
        "      if new_word != '':\n",
        "         new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        " words = lower_text.apply(lemmatize_text)\n",
        " words = words.apply(remove_punctuation)\n",
        " return pd.DataFrame(words)\n",
        "\n",
        "prep_tweets = preprocess_data(tweets['textprep'])\n",
        "tweets['textprep2'] = prep_tweets"
      ],
      "metadata": {
        "id": "2h8YHlnuAQir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "lNTl9hkuAl3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "tweets['textprep2'] = tweets['textprep2'].apply(lambda x: [item for item in \\\n",
        "                                    x if item not in stop_words])"
      ],
      "metadata": {
        "id": "8UZ2A0OUAn43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "fjQ5QJAYAq6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#join tokenize data\n",
        "tweets['textprep_join'] = tweets['textprep2'].apply(lambda x: ' '.join(x))\n",
        "tweets"
      ],
      "metadata": {
        "id": "m-Hs3xcjAtij"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}